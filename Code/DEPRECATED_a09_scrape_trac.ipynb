{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pandas.api.types import union_categoricals\n",
    "from itertools import islice\n",
    "import re\n",
    "import addfips\n",
    "import requests\n",
    "import urllib\n",
    "import time\n",
    "DC_STATEHOOD = 1 # Enables DC to be included in the state list\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape data from TRAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load table that has all state codes and put state codes into dict\n",
    "state_table = requests.get(\"https://tracreports.org/phptools/immigration/newdetain/table_json.php?stat=count&fy=All&dimension=LEA_state&sort=keyasc\").json()\n",
    "\n",
    "state_keys = {}\n",
    "for x in state_table['data']:\n",
    "    state_keys[x['label']] = x['code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load table that has all the year codes and put year codes into dict\n",
    "year_table = requests.get(\"https://tracreports.org/phptools/immigration/newdetain/table_json.php?stat=count&LEA_state=All&dimension=fy&sort=keydesc\").json()\n",
    "\n",
    "year_keys = {}\n",
    "for x in year_table['data']:\n",
    "    year_keys[x['label']] = x['code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fy_rows = []\n",
    "\n",
    "# Load county-facility table for each fiscal year, put response table into list\n",
    "for year, year_code in year_keys.items():\n",
    "    county_fac_table = requests.get(f\"https://tracreports.org/phptools/immigration/newdetain/table_json.php?stat=count&fy={year_code}&LEA_state=All&dimension=trac_fac_name_county&sort=keyasc\").json()\n",
    "\n",
    "    for entry in county_fac_table['data']:\n",
    "        row = [year, entry['code'], entry['label'], entry['value']]\n",
    "        fy_rows.append(row)\n",
    "\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put responses from list into dataframe\n",
    "detainer_counts = pd.DataFrame(fy_rows, columns = ['year', 'facility_code', 'facility_name', 'detainers_issued'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add FIPS code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract state and county name from facility name, to facilitate adding FIPS codes using the addFIPS package\n",
    "detainer_counts['county_name'] = detainer_counts['facility_name'].str.split(',').str[0]\n",
    "detainer_counts['county_name'] = detainer_counts['county_name'].astype(str)\n",
    "detainer_counts['state_abbrev'] = detainer_counts['facility_name'].str.extract(r'(,\\s\\D{2})')\n",
    "detainer_counts['state_abbrev'] = detainer_counts['state_abbrev'].astype(str) # Some entries don't have state names, i.e., aggregates\n",
    "detainer_counts['state_abbrev'] = detainer_counts['state_abbrev'].str.strip(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FIPS code\n",
    "af = addfips.AddFIPS()\n",
    "detainer_counts['fips_from_addfips'] = detainer_counts.apply(lambda x: af.get_county_fips(x['county_name'], state=x['state_abbrev']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the unmatched locations?\n",
    "unmatched_df = detainer_counts[detainer_counts['fips_from_addfips'].isna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Google Find Places API to get county for the remaining facilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by finding the Place ID for each location using Find Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For counties with 'None', these are for states that potentially don't have counties\n",
    "# Match using facility name instead\n",
    "unmatched_df['location_name'] = unmatched_df['county_name']\n",
    "unmatched_df.loc[unmatched_df['county_name']=='None', 'location_name'] = unmatched_df.loc[unmatched_df['county_name']=='None', 'facility_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ID for each row to link with API request responses\n",
    "unmatched_df['id'] = unmatched_df.reset_index().index.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split API calls into chunks of 100\n",
    "unmatched_df['chunk'] = unmatched_df['id'].astype(int)//100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google maps API key from my account\n",
    "# Import API key stored in text file\n",
    "with open(\"../tools/google_places_api_key.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "api_key = lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Base url to call Find Place API\n",
    "# base_url = \"https://maps.googleapis.com/maps/api/place/findplacefromtext/json?\"\n",
    "\n",
    "# for c in range(0, 8):\n",
    "#     unmatched_chunk = unmatched_df[unmatched_df['chunk'] == c]\n",
    "\n",
    "#     # Dict to store API responses\n",
    "#     api_placeid_dict = {}\n",
    "\n",
    "#     for ind in range(0, len(unmatched_chunk)):\n",
    "#         row = unmatched_chunk.iloc[ind]\n",
    "#         id = row['id']\n",
    "#         state_name = row['state_abbrev']\n",
    "#         place_name = row['location_name']\n",
    "#         name_to_search = place_name + ', ' + state_name\n",
    "\n",
    "#         print(id, name_to_search)\n",
    "\n",
    "#         # Create API request\n",
    "#         # URL'ed location name we want to search\n",
    "#         input = urllib.parse.quote(name_to_search) # Encode place name as URL string\n",
    "#         request_url = base_url + \"input=\" + input + \"&inputtype=textquery\" + \"&fields=place_id\" + \"&key=\" + api_key\n",
    "\n",
    "#         payload = {}\n",
    "#         headers = {}\n",
    "\n",
    "#         # Sleep one second between each API call\n",
    "#         time.sleep(1)\n",
    "\n",
    "#         # Make API call\n",
    "#         response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#         response_json = response.json()\n",
    "        \n",
    "#         # If API call is successful, then place response result into dict\n",
    "#         if response_json['status']=='OK':\n",
    "#             print('Successful')\n",
    "#             api_placeid_dict[id] = response_json\n",
    "#         else:\n",
    "#             # If API call is unsuccessful, then wait 5 seconds and retry\n",
    "#             print('NOT successful, retrying')\n",
    "#             time.sleep(5)\n",
    "#             response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#             response_json = response.json()\n",
    "\n",
    "#             if response_json['status']=='OK':\n",
    "#                 print('Retry successful')\n",
    "#                 api_placeid_dict[id] = response_json\n",
    "#             else:\n",
    "#                 error_type = response_json['status']\n",
    "#                 print('Retry unsuccessful, error: ' + error_type)\n",
    "\n",
    "#     # Save API request results as JSON\n",
    "#     with open(f'json/trac_placeid_api_request_result_chunk_{c}.json', 'w') as f:\n",
    "#         json.dump(api_placeid_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON of API responses and put into DataFrame\n",
    "api_placeid_dict = {}\n",
    "for c in range(0, 8):\n",
    "    with open(f'json/trac_placeid_api_request_result_chunk_{c}.json', 'r') as infile:\n",
    "        api_dict = json.load(infile)\n",
    "\n",
    "    api_placeid_dict = api_placeid_dict | api_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put place IDs into DataFrame\n",
    "api_placeid_df = pd.DataFrame(columns=['id', 'placeid'])\n",
    "\n",
    "for id, response in api_placeid_dict.items():\n",
    "    number_of_candidates = len(response['candidates'])\n",
    "    for response_ind in range(0, number_of_candidates):\n",
    "        placeid = response['candidates'][response_ind]['place_id']\n",
    "        api_placeid_df.loc[len(api_placeid_df)] = [id, placeid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split API calls into chunks of 100\n",
    "api_placeid_df['chunk'] = api_placeid_df['id'].astype(int)//100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use Place details API to get county names\n",
    "# base_url = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "# for c in range(0, 10):\n",
    "#     api_placeid_chunk = api_placeid_df[api_placeid_df['chunk'] == c]\n",
    "#     api_place_details_dict = {}\n",
    "\n",
    "#     # Iterate over each place ID\n",
    "#     for index, row in api_placeid_chunk.iterrows():\n",
    "#         print(row['id'], row['placeid'])\n",
    "\n",
    "#         # Create API request\n",
    "#         input = row['placeid']\n",
    "#         request_url = base_url + \"place_id=\" + input + \"&key=\" + api_key\n",
    "\n",
    "#         payload = {}\n",
    "#         headers = {}\n",
    "\n",
    "#         response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#         response_json = response.json()\n",
    "\n",
    "#         # If API call is successful, then place response result into dict\n",
    "#         if response_json['status']=='OK':\n",
    "#             print('Successful')\n",
    "#             api_place_details_dict[input] = response_json\n",
    "#         else:\n",
    "#             # If API call is unsuccessful, then wait 5 seconds and retry\n",
    "#             print('NOT successful, retrying')\n",
    "#             time.sleep(5)\n",
    "#             response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#             response_json = response.json()\n",
    "\n",
    "#             if response_json['status']=='OK':\n",
    "#                 print('Retry successful')\n",
    "#                 api_place_details_dict[input] = response_json\n",
    "#             else:\n",
    "#                 error_type = response_json['status']\n",
    "#                 print('Retry unsuccessful, error: ' + error_type)\n",
    "\n",
    "#     # Save API request results as JSON\n",
    "#     with open(f'json/trac_place_details_api_request_result_chunk_{c}.json', 'w') as f:\n",
    "#         json.dump(api_place_details_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON of API responses and put into DataFrame\n",
    "api_place_details_dict = {}\n",
    "for c in range(0, 1):\n",
    "    with open(f'json/trac_place_details_api_request_result_chunk_{c}.json', 'r') as infile:\n",
    "        api_dict = json.load(infile)\n",
    "\n",
    "    api_place_details_dict = api_place_details_dict | api_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store county name from place details into dictionary (store state names too as there may be incorrect states)\n",
    "county_name_dict = {}\n",
    "state_name_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information we want from API response\n",
    "for placeid, response in api_place_details_dict.items():\n",
    "    n_responses = len(response['results'])\n",
    "\n",
    "    for response_ind in range(0, n_responses):\n",
    "        individual_response = response['results'][response_ind]\n",
    "        response_address_components_list = individual_response['address_components']\n",
    "        n_components = len(response_address_components_list)\n",
    "\n",
    "        for component_ind in range(0, n_components):\n",
    "            component_dict = response_address_components_list[component_ind]\n",
    "            component_type =  component_dict['types'][0]\n",
    "\n",
    "            if component_type == 'administrative_area_level_2':\n",
    "                county_name = component_dict['long_name']\n",
    "                county_name_dict[placeid] = county_name\n",
    "            \n",
    "            if component_type == 'administrative_area_level_1':\n",
    "                state_name = component_dict['long_name']\n",
    "                state_name_dict[placeid] = state_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add county and state name columns to Place ID\n",
    "api_placeid_df['county_name_api'] = api_placeid_df['placeid'].map(county_name_dict)\n",
    "api_placeid_df['state_name_api'] = api_placeid_df['placeid'].map(state_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of these multiple responses per place name are in the same county, so we can collapse those\n",
    "api_placeid_df = api_placeid_df.drop_duplicates(subset = ['id', 'county_name_api', 'state_name_api'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the remainder, manually resolve\n",
    "api_placeid_df = api_placeid_df.merge(unmatched_df[['state_abbrev', 'location_name' , 'id']], how = 'left', on = ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are unmatched independent cities\n",
    "api_placeid_df.loc[(api_placeid_df['location_name'] == 'Galax Police Department') & (api_placeid_df['state_abbrev'] == 'VA'), 'county_name_api'] = 'Galax'\n",
    "api_placeid_df.loc[(api_placeid_df['location_name'] == 'Lynchburg Pol Dept') & (api_placeid_df['state_abbrev'] == 'VA'), 'county_name_api'] = 'Lynchburg'\n",
    "\n",
    "api_placeid_df.loc[(api_placeid_df['location_name'] == 'City of Baltimore') & (api_placeid_df['state_abbrev'] == 'MD'), 'county_name_api'] = 'Baltimore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recollapse after fixing\n",
    "api_placeid_df = api_placeid_df.drop_duplicates(subset = ['id', 'county_name_api', 'state_abbrev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get FIPS codes using addFIPS\n",
    "af = addfips.AddFIPS()\n",
    "api_placeid_df = api_placeid_df[~api_placeid_df['county_name_api'].isna()].copy()\n",
    "api_placeid_df['fips_api'] = api_placeid_df.apply(lambda x: af.get_county_fips(x['county_name_api'], state=x['state_abbrev']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recollapse back into individual entries (some entries had multiple places per entry)\n",
    "trac_api_df = unmatched_df.merge(api_placeid_df[['id', 'county_name_api', 'fips_api']], how = 'left', on = ['id'])\n",
    "trac_api_df = trac_api_df[~trac_api_df['fips_api'].isna()].copy()\n",
    "trac_api_df = trac_api_df.drop_duplicates(subset = ['state_abbrev', 'facility_code', 'fips_api'])\n",
    "trac_api_df = trac_api_df.groupby(['state_abbrev', 'facility_code']).agg({'fips_api': lambda x: \",\".join(x)}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FIPS from API back to original list of worksites\n",
    "county_df = detainer_counts.merge(trac_api_df, how = 'left', on = ['state_abbrev', 'facility_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "county_df = county_df.fillna(value='')\n",
    "county_df['fips'] = county_df['fips_from_addfips']\n",
    "county_df.loc[county_df['fips'] == '', 'fips'] = county_df.loc[county_df['fips'] == '', 'fips_api']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a few more entries we can manually resolve\n",
    "# Brookhaven PD in Delaware County is in Pennsylvania, not Georgia\n",
    "county_df.loc[(county_df['facility_code'] == '350') & (county_df['state_abbrev'] == 'PA'), 'fips'] = af.get_county_fips('Delaware County', 'Pennsylvania')\n",
    "\n",
    "# Kemper-Neshoba Regional Correctional Facility, Kemper County is in Mississippi, not Michigan\n",
    "county_df.loc[(county_df['facility_code'] == '2984') & (county_df['state_abbrev'] == 'MS'), 'fips'] = af.get_county_fips('Kemper County', 'Mississippi')\n",
    "\n",
    "# Suffolk County - Ny State Police Farmingdale is in New York, not Arkansas\n",
    "county_df.loc[(county_df['facility_code'] == '3260') & (county_df['state_abbrev'] == 'NY'), 'fips'] = af.get_county_fips('Suffolk County', 'New York')\n",
    "\n",
    "# The rest are in U.S. Territories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export binary\n",
    "county_df = county_df.drop(columns = ['fips_from_addfips', 'fips_api'])\n",
    "county_df = county_df.astype(str)\n",
    "county_df.to_parquet(\"../binaries/trac_detainer_counts.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h-2a-paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

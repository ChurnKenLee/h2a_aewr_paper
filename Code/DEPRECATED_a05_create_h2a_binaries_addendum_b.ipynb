{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pandas.api.types import union_categoricals\n",
    "from itertools import islice\n",
    "import re\n",
    "import addfips\n",
    "import requests\n",
    "import urllib\n",
    "import time\n",
    "DC_STATEHOOD = 1 # Enables DC to be included in the state list\n",
    "import us\n",
    "import pickle\n",
    "import rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Census geographic codes file\n",
    "census_county = pd.read_csv(\"../Data/census_geography_codes/national_county2020.txt\", sep='|', dtype = 'string', keep_default_na=False).apply(lambda x: x.str.upper())\n",
    "census_countysub = pd.read_csv(\"../Data/census_geography_codes/national_cousub2020.txt\", sep='|', dtype = 'string', keep_default_na=False).apply(lambda x: x.str.upper())\n",
    "census_place = pd.read_csv(\"../Data/census_geography_codes/national_place2020.txt\", sep='|', dtype = 'string', keep_default_na=False).apply(lambda x: x.str.upper())\n",
    "census_placebycounty = pd.read_csv(\"../Data/census_geography_codes/national_place_by_county2020.txt\", sep='|', dtype = 'string', keep_default_na=False).apply(lambda x: x.str.upper())\n",
    "census_zip = pd.read_csv(\"../Data/census_geography_codes/tab20_zcta520_county20_natl.txt\", sep='|', dtype = 'string', keep_default_na=False).apply(lambda x: x.str.upper())\n",
    "\n",
    "# Add FIPS column\n",
    "census_county['fips'] = census_county['STATEFP'] + census_county['COUNTYFP']\n",
    "census_placebycounty['fips'] = census_placebycounty['STATEFP'] + census_placebycounty['COUNTYFP']\n",
    "census_zip['fips'] = census_zip['GEOID_COUNTY_20']\n",
    "\n",
    "# There may be places and ZIP codes that map to multiple counties; collapse these into unique entries\n",
    "census_county = census_county[['STATE', 'COUNTYNAME', 'fips']]\n",
    "census_place_agg = census_placebycounty.groupby(['STATE', 'COUNTYNAME', 'PLACENAME']).agg({'fips':lambda x: \",\".join(x)}).reset_index()\n",
    "census_zip_agg = census_zip.groupby(['GEOID_ZCTA5_20']).agg({'fips':lambda x: \",\".join(x)}).reset_index()\n",
    "\n",
    "# Drop empty entries\n",
    "census_county = census_county[census_county['COUNTYNAME'] != '']\n",
    "census_place_agg = census_place_agg[census_place_agg['PLACENAME'] != '']\n",
    "census_zip_agg = census_zip_agg[census_zip_agg['GEOID_ZCTA5_20'] != '']\n",
    "census_zip_agg = census_zip_agg.rename(columns = {'GEOID_ZCTA5_20':'zip', 'fips':'fips_from_zip'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of H-2A program Addendum B files from the DOL-OFLC\n",
    "h2a_file_name_dict = {\n",
    "    '2020':'H-2A_FY2020_AddendumB_Employment.xlsx',\n",
    "    '2021':'H-2A_Addendum_B_Employment_FY2021.xlsx',\n",
    "    '2022':'H-2A_Addendum_B_Employment_Record_FY2022_Q4.xlsx'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common set of variables we want from every fiscal year, and their types\n",
    "h2a_dtype_dict = {}\n",
    "\n",
    "h2a_dtype_dict['2020'] = {\n",
    "    'CASE_NUMBER':'string',\n",
    "    'NAME_OF_AGRICULTURAL_BUSINESS':'string',\n",
    "    'PLACE_OF_EMPLOYMENT_CITY':'string',\n",
    "    'PLACE_OF_EMPLOYMENT_STATE':'string',\n",
    "    'PLACE_OF_EMPLOYMENT_POSTAL_CODE':'string',\n",
    "    'TOTAL_WORKERS':'string'\n",
    "}\n",
    "\n",
    "h2a_dtype_dict['2021'] = h2a_dtype_dict['2020']\n",
    "h2a_dtype_dict['2022'] = h2a_dtype_dict['2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define set of common names for concatenating\n",
    "h2a_rename_dict = {\n",
    "    'CASE_NUMBER':'case_number',\n",
    "    'NAME_OF_AGRICULTURAL_BUSINESS':'business_name',\n",
    "    'PLACE_OF_EMPLOYMENT_CITY':'worksite_city',\n",
    "    'PLACE_OF_EMPLOYMENT_STATE':'worksite_state',\n",
    "    'PLACE_OF_EMPLOYMENT_POSTAL_CODE':'worksite_zip',\n",
    "    'TOTAL_WORKERS':'total_h2a_workers_requested'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h2a_df_dict = {}\n",
    "# for year, file_name in h2a_file_name_dict.items():\n",
    "#     h2a_path = Path(f\"../Data/h2a/{file_name}\")\n",
    "#     print(h2a_path)\n",
    "\n",
    "#     dtype_dict = h2a_dtype_dict[year]\n",
    "#     col_list = list(dtype_dict.keys())\n",
    "#     h2a_df_dict[year] = pd.read_excel(h2a_path, usecols = col_list, dtype = dtype_dict, parse_dates=False)\n",
    "\n",
    "# # Pickling\n",
    "# with open(\"h2a_addendum_b_pickle\", \"wb\") as fp:\n",
    "#     pickle.dump(h2a_df_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickling\n",
    "with open(\"h2a_addendum_b_pickle\", \"rb\") as fp:\n",
    "    h2a_df_dict = pickle.load(fp)\n",
    "\n",
    "h2a_df = pd.DataFrame()\n",
    "for year, df in h2a_df_dict.items():\n",
    "    df = df.rename(columns = h2a_rename_dict)\n",
    "    df['fiscal_year'] = year\n",
    "    h2a_df = pd.concat([h2a_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define consistent NAs, convert all entries to uppercase\n",
    "h2a_df = h2a_df.fillna(value='').apply(lambda x: x.str.upper())\n",
    "h2a_df['worksite_zip'] = h2a_df['worksite_zip'].str.replace(pat = ' ', repl = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match FIPS codes for worksite locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2a_worksite_locations = h2a_df[['worksite_city', 'worksite_state', 'worksite_zip']]\n",
    "\n",
    "# Drop duplicated entries\n",
    "h2a_worksite_locations = h2a_worksite_locations.drop_duplicates()\n",
    "h2a_worksite_locations['city'] = h2a_worksite_locations['worksite_city']\n",
    "h2a_worksite_locations['zip'] = h2a_worksite_locations['worksite_zip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up ZIP codes\n",
    "h2a_worksite_locations.loc[((h2a_worksite_locations['zip'].str.len() < 5) & (h2a_worksite_locations['zip'] != '')), 'zip'] = h2a_worksite_locations.loc[((h2a_worksite_locations['zip'].str.len() < 5) & (h2a_worksite_locations['zip'] != '')), 'zip'].str.pad(width = 5, side = 'left', fillchar = '0')\n",
    "h2a_worksite_locations.loc[(h2a_worksite_locations['zip'].str.len() > 5), 'zip'] = h2a_worksite_locations.loc[(h2a_worksite_locations['zip'].str.len() > 5), 'zip'].str.slice(start = 0, stop = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add state abbreviation where possible\n",
    "h2a_worksite_locations['state'] = h2a_worksite_locations['worksite_state'].apply(lambda x: us.states.lookup(x).abbr if ((x!='') & (x!='FEDERATED STATES OF MICRONESIA')) else '')\n",
    "\n",
    "# For entries without state but has ZIP code, add state that way\n",
    "h2a_worksite_locations = h2a_worksite_locations.merge(census_zip_agg, how='left', on=['zip'])\n",
    "h2a_worksite_locations['state_fip_from_zip'] = h2a_worksite_locations['fips_from_zip'].str.slice(start = 0, stop = 2)\n",
    "h2a_worksite_locations.loc[h2a_worksite_locations['state_fip_from_zip'].isna(), 'state_fip_from_zip'] = ''\n",
    "h2a_worksite_locations['state_from_zip'] = h2a_worksite_locations['state_fip_from_zip'].apply(lambda x: us.states.lookup(x).abbr if x!='' else '')\n",
    "h2a_worksite_locations.loc[h2a_worksite_locations['state'] == '', 'state'] = h2a_worksite_locations.loc[h2a_worksite_locations['state'] == '', 'state_from_zip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for fuzzy string matching\n",
    "def fuzz_search(census_df, census_col, state_to_search, name_to_match):\n",
    "\n",
    "    def fuzz_match(x, y):\n",
    "        return rapidfuzz.fuzz.WRatio(x, y)\n",
    "    \n",
    "    state_df = census_df[census_df['STATE'] == state_to_search].copy()\n",
    "    state_df['score'] = state_df[census_col].apply(lambda x: fuzz_match(x, name_to_match))\n",
    "    \n",
    "    state_df = state_df.sort_values('score')\n",
    "    \n",
    "    max_score_row = state_df[state_df['score'] == state_df['score'].max()].reset_index()\n",
    "\n",
    "    # Best match\n",
    "    if len(max_score_row) >= 1:\n",
    "        fips = str(max_score_row['fips'][0])\n",
    "        score = str(max_score_row['score'][0])\n",
    "        census_name = (max_score_row[census_col][0])\n",
    "        return(fips, score, census_name)\n",
    "    else:\n",
    "        return('', '', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get matches and match score using city\n",
    "census_df = census_placebycounty\n",
    "census_col = 'PLACENAME'\n",
    "fuzzy_result_df = h2a_worksite_locations.apply(lambda x: fuzz_search(census_df, census_col, x.state, x.city), axis=1, result_type='expand')\n",
    "fuzzy_result_df = fuzzy_result_df.rename(columns = {0:'fips_from_city', 1:'score_from_city', 2:'census_name_city'})\n",
    "fuzzy_result_df['score_from_city'] = pd.to_numeric(fuzzy_result_df['score_from_city'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It appears 85.5 is a good cutoff\n",
    "fuzzy_result_df.loc[fuzzy_result_df['score_from_city'] < 85.6, ['fips_from_city']] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine match list back in\n",
    "h2a_worksite_locations = pd.concat([h2a_worksite_locations, fuzzy_result_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined FIPS, and use Google Places API for the rest\n",
    "# Write function with logic for choosing FIPS\n",
    "def fips_choice(county_fips, zip_fips, city_fips):\n",
    "    if (zip_fips != ''):\n",
    "        return(zip_fips)\n",
    "\n",
    "    if (county_fips != ''):\n",
    "        return(county_fips)\n",
    "    \n",
    "    if (city_fips != ''):\n",
    "        return(city_fips)\n",
    "    \n",
    "    else:\n",
    "        return('')\n",
    "    \n",
    "h2a_worksite_locations = h2a_worksite_locations.fillna('')\n",
    "h2a_worksite_locations['fips_from_census'] = h2a_worksite_locations.apply(lambda x: fips_choice('', x.fips_from_zip, x.fips_from_city), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining locations, find county using Google's Places API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by finding the Place ID for each location using Find Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2a_unmatched_census = h2a_worksite_locations[(h2a_worksite_locations['fips_from_census'] == '') & (h2a_worksite_locations['worksite_city'] != '') & (h2a_worksite_locations['worksite_state'] != '')][['worksite_city', 'worksite_state', 'worksite_zip', 'city', 'state', 'zip']]\n",
    "h2a_unmatched_census['state_name'] = h2a_unmatched_census['worksite_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ID for each row to link with API request responses\n",
    "h2a_unmatched_census['id'] = h2a_unmatched_census.reset_index().index.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split API calls into chunks of 100\n",
    "h2a_unmatched_census['chunk'] = h2a_unmatched_census['id'].astype(int)//100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google maps API key from my account\n",
    "# Import API key stored in text file\n",
    "with open(\"../tools/google_places_api_key.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "api_key = lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Base url to call Find Place API\n",
    "# base_url = \"https://maps.googleapis.com/maps/api/place/findplacefromtext/json?\"\n",
    "\n",
    "# for c in range(0, 8):\n",
    "#     h2a_chunk = h2a_unmatched_census[h2a_unmatched_census['chunk'] == c]\n",
    "\n",
    "#     # Dict to store API responses\n",
    "#     api_placeid_dict = {}\n",
    "\n",
    "#     for ind in range(0, len(h2a_chunk)):\n",
    "#         row = h2a_chunk.iloc[ind]\n",
    "#         id = row['id']\n",
    "#         state_name = row['state_name']\n",
    "#         place_name = row['city']\n",
    "#         name_to_search = place_name + ', ' + state_name\n",
    "\n",
    "#         print(id, name_to_search)\n",
    "\n",
    "#         # Create API request\n",
    "#         # URL'ed location name we want to search\n",
    "#         input = urllib.parse.quote(name_to_search) # Encode place name as URL string\n",
    "#         request_url = base_url + \"input=\" + input + \"&inputtype=textquery\" + \"&fields=place_id\" + \"&key=\" + api_key\n",
    "\n",
    "#         payload = {}\n",
    "#         headers = {}\n",
    "\n",
    "#         # Sleep one second between each API call\n",
    "#         time.sleep(1)\n",
    "\n",
    "#         # Make API call\n",
    "#         response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#         response_json = response.json()\n",
    "        \n",
    "#         # If API call is successful, then place response result into dict\n",
    "#         if response_json['status']=='OK':\n",
    "#             print('Successful')\n",
    "#             api_placeid_dict[id] = response_json\n",
    "#         else:\n",
    "#             # If API call is unsuccessful, then wait 5 seconds and retry\n",
    "#             print('NOT successful, retrying')\n",
    "#             time.sleep(5)\n",
    "#             response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#             response_json = response.json()\n",
    "\n",
    "#             if response_json['status']=='OK':\n",
    "#                 print('Retry successful')\n",
    "#                 api_placeid_dict[id] = response_json\n",
    "#             else:\n",
    "#                 error_type = response_json['status']\n",
    "#                 print('Retry unsuccessful, error: ' + error_type)\n",
    "\n",
    "#     # Save API request results as JSON\n",
    "#     with open(f'json/addendum_b_placeid_api_request_result_chunk_{c}.json', 'w') as f:\n",
    "#         json.dump(api_placeid_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the Place ID to find the county name of each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON of API responses and put into DataFrame\n",
    "api_placeid_dict = {}\n",
    "for c in range(0, 8):\n",
    "    with open(f'json/addendum_b_placeid_api_request_result_chunk_{c}.json', 'r') as infile:\n",
    "        api_dict = json.load(infile)\n",
    "\n",
    "    api_placeid_dict = api_placeid_dict | api_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put place IDs into DataFrame\n",
    "api_placeid_df = pd.DataFrame(columns=['id', 'placeid'])\n",
    "\n",
    "for id, response in api_placeid_dict.items():\n",
    "    number_of_candidates = len(response['candidates'])\n",
    "    for response_ind in range(0, number_of_candidates):\n",
    "        placeid = response['candidates'][response_ind]['place_id']\n",
    "        api_placeid_df.loc[len(api_placeid_df)] = [id, placeid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split API calls into chunks of 100\n",
    "api_placeid_df['chunk'] = api_placeid_df['id'].astype(int)//100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use Place details API to get county names\n",
    "# base_url = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "# for c in range(0, 10):\n",
    "#     api_placeid_chunk = api_placeid_df[api_placeid_df['chunk'] == c]\n",
    "#     api_place_details_dict = {}\n",
    "\n",
    "#     # Iterate over each place ID\n",
    "#     for index, row in api_placeid_chunk.iterrows():\n",
    "#         print(row['id'], row['placeid'])\n",
    "\n",
    "#         # Create API request\n",
    "#         input = row['placeid']\n",
    "#         request_url = base_url + \"place_id=\" + input + \"&key=\" + api_key\n",
    "\n",
    "#         payload = {}\n",
    "#         headers = {}\n",
    "\n",
    "#         response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#         response_json = response.json()\n",
    "\n",
    "#         # If API call is successful, then place response result into dict\n",
    "#         if response_json['status']=='OK':\n",
    "#             print('Successful')\n",
    "#             api_place_details_dict[input] = response_json\n",
    "#         else:\n",
    "#             # If API call is unsuccessful, then wait 5 seconds and retry\n",
    "#             print('NOT successful, retrying')\n",
    "#             time.sleep(5)\n",
    "#             response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#             response_json = response.json()\n",
    "\n",
    "#             if response_json['status']=='OK':\n",
    "#                 print('Retry successful')\n",
    "#                 api_place_details_dict[input] = response_json\n",
    "#             else:\n",
    "#                 error_type = response_json['status']\n",
    "#                 print('Retry unsuccessful, error: ' + error_type)\n",
    "\n",
    "#     # Save API request results as JSON\n",
    "#     with open(f'json/addendum_b_place_details_api_request_result_chunk_{c}.json', 'w') as f:\n",
    "#         json.dump(api_place_details_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON of API responses and put into DataFrame\n",
    "api_place_details_dict = {}\n",
    "for c in range(0, 10):\n",
    "    with open(f'json/addendum_b_place_details_api_request_result_chunk_{c}.json', 'r') as infile:\n",
    "        api_dict = json.load(infile)\n",
    "\n",
    "    api_place_details_dict = api_place_details_dict | api_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store county name from place details into dictionary (store state names too as there may be incorrect states)\n",
    "county_name_dict = {}\n",
    "state_name_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information we want from API response\n",
    "for placeid, response in api_place_details_dict.items():\n",
    "    n_responses = len(response['results'])\n",
    "\n",
    "    for response_ind in range(0, n_responses):\n",
    "        individual_response = response['results'][response_ind]\n",
    "        response_address_components_list = individual_response['address_components']\n",
    "        n_components = len(response_address_components_list)\n",
    "\n",
    "        for component_ind in range(0, n_components):\n",
    "            component_dict = response_address_components_list[component_ind]\n",
    "            component_type =  component_dict['types'][0]\n",
    "\n",
    "            if component_type == 'administrative_area_level_2':\n",
    "                county_name = component_dict['long_name']\n",
    "                county_name_dict[placeid] = county_name\n",
    "            \n",
    "            if component_type == 'administrative_area_level_1':\n",
    "                state_name = component_dict['long_name']\n",
    "                state_name_dict[placeid] = state_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add county and state name columns to Place ID\n",
    "api_placeid_df['county_name_api'] = api_placeid_df['placeid'].map(county_name_dict)\n",
    "api_placeid_df['state_name_api'] = api_placeid_df['placeid'].map(state_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of these multiple responses per place name are in the same county, so we can collapse those\n",
    "api_placeid_df = api_placeid_df.drop_duplicates(subset = ['id', 'county_name_api', 'state_name_api'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the remainder, manually resolve\n",
    "api_placeid_df = api_placeid_df.merge(h2a_unmatched_census[['city', 'state_name', 'id']], how = 'left', on = ['id'])\n",
    "multiple_response = api_placeid_df[api_placeid_df.duplicated(subset=['id'], keep=False)]\n",
    "multiple_response.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_placeid_df.loc[(api_placeid_df['city'] == 'LA SELLE') & (api_placeid_df['state_name'] == 'ILLINOIS'), 'county_name_api'] = 'LaSalle County'\n",
    "\n",
    "api_placeid_df.loc[(api_placeid_df['city'] == 'HARRISON TWP') & (api_placeid_df['state_name'] == 'INDIANA'), 'county_name_api'] = None # Ambiguous\n",
    "\n",
    "api_placeid_df.loc[(api_placeid_df['city'] == 'HARRISON TWP') & (api_placeid_df['state_name'] == 'INDIANA'), 'county_name_api'] = None\n",
    "\n",
    "api_placeid_df.loc[(api_placeid_df['city'] == 'TOWNSHIP') & (api_placeid_df['state_name'] == 'ILLINOIS'), 'county_name_api'] = None\n",
    "\n",
    "api_placeid_df.loc[(api_placeid_df['city'] == 'OWNSHIP') & (api_placeid_df['state_name'] == 'ILLINOIS'), 'county_name_api'] = None\n",
    "\n",
    "api_placeid_df.loc[(api_placeid_df['city'] == 'JORDAN/LIBERTY') & (api_placeid_df['state_name'] == 'INDIANA'), 'county_name_api'] = None\n",
    "\n",
    "api_placeid_df.loc[(api_placeid_df['city'] == 'PHOENIX AND SURROUNDING CITIES ') & (api_placeid_df['state_name'] == 'ARIZONA'), 'county_name_api'] = 'Maricopa County'\n",
    "\n",
    "api_placeid_df.loc[(api_placeid_df['city'] == 'BROOKFIELD TWPS') & (api_placeid_df['state_name'] == 'MICHIGAN'), 'county_name_api'] = None\n",
    "\n",
    "api_placeid_df.loc[(api_placeid_df['city'] == 'FAIRHAVEN TWPS') & (api_placeid_df['state_name'] == 'MICHIGAN'), 'county_name_api'] = 'Huron County'\n",
    "\n",
    "api_placeid_df.loc[(api_placeid_df['city'] == 'CYPRESS') & (api_placeid_df['state_name'] == 'SOUTH CAROLINA'), 'county_name_api'] = None\n",
    "\n",
    "api_placeid_df.loc[(api_placeid_df['city'] == 'CRYSTAL TWSP') & (api_placeid_df['state_name'] == 'MICHIGAN'), 'county_name_api'] = None\n",
    "\n",
    "api_placeid_df.loc[(api_placeid_df['city'] == 'LEROY TWSP') & (api_placeid_df['state_name'] == 'MICHIGAN'), 'county_name_api'] = None\n",
    "\n",
    "api_placeid_df.loc[(api_placeid_df['city'] == 'GREGREENFIELD') & (api_placeid_df['state_name'] == 'CALIFORNIA'), 'county_name_api'] = None\n",
    "\n",
    "api_placeid_df.loc[(api_placeid_df['city'] == 'NONE') & (api_placeid_df['state_name'] == 'SOUTH DAKOTA'), 'county_name_api'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recollapse after fixing\n",
    "api_placeid_df = api_placeid_df.drop_duplicates(subset = ['id', 'county_name_api', 'state_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get FIPS codes using addFIPS\n",
    "af = addfips.AddFIPS()\n",
    "api_placeid_df = api_placeid_df[~api_placeid_df['county_name_api'].isna()].copy()\n",
    "api_placeid_df['fips_api'] = api_placeid_df.apply(lambda x: af.get_county_fips(x['county_name_api'], state=x['state_name']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop API results that don't match states\n",
    "api_placeid_df = api_placeid_df[api_placeid_df['state_name'] == api_placeid_df['state_name_api'].str.upper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recollapse back into individual entries (some entries had multiple places per entry)\n",
    "h2a_api_df = h2a_unmatched_census.merge(api_placeid_df[['id', 'county_name_api', 'fips_api']], how = 'left', on = ['id'])\n",
    "h2a_api_df = h2a_api_df[~h2a_api_df['fips_api'].isna()].copy()\n",
    "h2a_api_df = h2a_api_df.groupby(['worksite_city', 'worksite_state', 'worksite_zip']).agg({'fips_api': lambda x: \",\".join(x)}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FIPS from API back to original list of worksites\n",
    "h2a_worksite_locations = h2a_worksite_locations.merge(h2a_api_df, how = 'left', on = ['worksite_city', 'worksite_state', 'worksite_zip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "h2a_worksite_locations = h2a_worksite_locations.fillna(value='')\n",
    "h2a_worksite_locations['fips'] = h2a_worksite_locations['fips_from_census']\n",
    "h2a_worksite_locations.loc[h2a_worksite_locations['fips'] == '', 'fips'] = h2a_worksite_locations.loc[h2a_worksite_locations['fips'] == '', 'fips_api']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FIPS to H-2A entries based on worksites\n",
    "h2a_df_final = h2a_df.merge(h2a_worksite_locations[['worksite_city', 'worksite_state', 'worksite_zip', 'fips']], how = 'left', on = ['worksite_city', 'worksite_state', 'worksite_zip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export binary\n",
    "h2a_df_final.to_parquet(\"../binaries/h2a_addendum_b_with_fips.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h2a_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

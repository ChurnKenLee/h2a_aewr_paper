{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\churn_5stexhd\\Utilities\\anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numba needs NumPy 1.22 or greater. Got NumPy 1.21.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SearchEngine' from 'uszipcode' (c:\\Users\\churn_5stexhd\\Utilities\\anaconda3\\lib\\site-packages\\uszipcode\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m DC_STATEHOOD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# Enables DC to be included in the state list\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mus\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muszipcode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SearchEngine\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SearchEngine' from 'uszipcode' (c:\\Users\\churn_5stexhd\\Utilities\\anaconda3\\lib\\site-packages\\uszipcode\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pandas.api.types import union_categoricals\n",
    "from itertools import islice\n",
    "import re\n",
    "import addfips\n",
    "import requests\n",
    "import urllib\n",
    "import time\n",
    "DC_STATEHOOD = 1 # Enables DC to be included in the state list\n",
    "import us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable list and types we want from H-2A application data\n",
    "h2a_dtype_dict = {\n",
    "    'FiscalYear': 'string',\n",
    "    'Matched': 'category',\n",
    "    'WORKSITE_STATE': 'category',\n",
    "    'WORKSITE_CITY': 'string',\n",
    "    'WORKSITE_COUNTY': 'string',\n",
    "    'ORIGINAL_WORKSITE_COUNTY': 'string',\n",
    "    'WORKSITE_POSTAL_CODE': 'string',\n",
    "    'EMPLOYER_NAME': 'string',\n",
    "    'new_EMPLOYER_NAME': 'string',\n",
    "    'CASE_NUMBER': 'string',\n",
    "    'CASE_STATUS': 'category',\n",
    "    'EMPLOYER_POSTAL_CODE': 'string',\n",
    "    'JOB_TITLE': 'string',\n",
    "    'NBR_WORKERS_REQUESTED': 'float32',\n",
    "    'NBR_WORKERS_CERTIFIED': 'float32',\n",
    "    'BASIC_NUMBER_OF_HOURS': 'float32',\n",
    "    'BASIC_RATE_OF_PAY': 'float32',\n",
    "    'BASIC_UNIT_OF_PAY': 'category',\n",
    "    'CERTIFICATION_BEGIN_DATE': 'string',\n",
    "    'CERTIFICATION_END_DATE': 'string',\n",
    "    'DECISION_DATE': 'string',\n",
    "    'CASE_RECEIVED_DATE': 'string',\n",
    "    'ORGANIZATION_FLAG': 'category',\n",
    "    'REQUESTED_START_DATE_OF_NEED': 'string',\n",
    "    'REQUESTED_END_DATE_OF_NEED': 'string',\n",
    "    'PRIMARY_CROP': 'string',\n",
    "    'LAWFIRM_NAME': 'string',\n",
    "    'SOC_CODE': 'category',\n",
    "    'SOC_TITLE': 'category',\n",
    "    'NAICS_CODE': 'category',\n",
    "    'FULL_TIME_POSITION': 'category',\n",
    "    'NATURE_OF_TEMPORARY_NEED': 'category',\n",
    "    'OVERTIME_RATE_FROM': 'float32',\n",
    "    'OVERTIME_RATE_TO': 'float32',\n",
    "    'EDUCATION_LEVEL': 'category',\n",
    "    'OTHER_EDU': 'category',\n",
    "    'SWA_NAME': 'string',\n",
    "    'JOB_IDNUMBER': 'string',\n",
    "    'JOB_START_DATE': 'string',\n",
    "    'JOB_END_DATE': 'string',\n",
    "    'TRADE_NAME_DBA': 'string',\n",
    "    'TYPE_OF_EMPLOYER_APPLICATION': 'category',\n",
    "    'JOB_ORDER_NUMBER': 'string',\n",
    "    'TOTAL_WORKERS_NEEDED': 'float32',\n",
    "    'SPECIAL_REQUIREMENTS': 'string',\n",
    "    'begin_year': 'string',\n",
    "    'end_year': 'string',\n",
    "    'case_year': 'string',\n",
    "    'Abbreviation': 'category',\n",
    "    'EMPLOYER_STATE': 'category',\n",
    "    'EMPLOYER_CITY': 'string'\n",
    "}\n",
    "\n",
    "col_list = list(h2a_dtype_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_rename_dict = {\n",
    "    'EMPLOYMENT_BEGIN_DATE': 'JOB_START_DATE',\n",
    "    'EMPLOYMENT_END_DATE': 'JOB_END_DATE',\n",
    "    'REQUESTED_BEGIN_DATE': 'REQUESTED_START_DATE_OF_NEED',\n",
    "    'REQUESTED_END_DATE': 'REQUESTED_END_DATE_OF_NEED',\n",
    "    'TOTAL_WORKERS_H2A_REQUESTED': 'NBR_WORKERS_REQUESTED',\n",
    "    'TOTAL_WORKERS_H2A_CERTIFIED': 'NBR_WORKERS_CERTIFIED'\n",
    "}\n",
    "\n",
    "small_h2a_dtype_dict = {\n",
    "    'CASE_NUMBER': 'string',\n",
    "    'CASE_STATUS': 'category',\n",
    "    'TOTAL_WORKERS_NEEDED': 'float32',\n",
    "    'TOTAL_WORKERS_H2A_REQUESTED': 'float32',\n",
    "    'TOTAL_WORKERS_H2A_CERTIFIED': 'float32',\n",
    "    'ANTICIPATED_NUMBER_OF_HOURS': 'float32',\n",
    "    'REQUESTED_BEGIN_DATE': 'string',\n",
    "    'REQUESTED_END_DATE': 'string',\n",
    "    'EMPLOYMENT_BEGIN_DATE': 'string',\n",
    "    'EMPLOYMENT_END_DATE': 'string',\n",
    "    'WORKSITE_STATE': 'category',\n",
    "    'WORKSITE_CITY': 'string',\n",
    "    'WORKSITE_COUNTY': 'string',\n",
    "    'WORKSITE_POSTAL_CODE': 'string'\n",
    "}\n",
    "\n",
    "small_col_list = list(small_h2a_dtype_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert from xlsx to parquet\n",
    "# df = pd.read_excel(\"../Data/h2a/H2A Final Worksite County Data.xlsx\", usecols=col_list, dtype=h2a_dtype_dict, parse_dates=False)\n",
    "# df.to_parquet(\"../binaries/h2a_raw.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2021 and 2022 numbers\n",
    "df_2021 = pd.read_excel(\"../Data/h2a/H-2A_Disclosure_Data_FY2021.xlsx\", usecols=small_col_list, dtype=small_h2a_dtype_dict, parse_dates=False)\n",
    "df_2021['FiscalYear'] = '2021'\n",
    "df_2022 = pd.read_excel(\"../Data/h2a/H-2A_Disclosure_Data_FY2022_Q4.xlsx\", usecols=small_col_list, dtype=small_h2a_dtype_dict, parse_dates=False)\n",
    "df_2022['FiscalYear'] = '2022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2021_2022 = pd.concat([df_2021, df_2022])\n",
    "df_2021_2022 = df_2021_2022.rename(columns = small_rename_dict)\n",
    "df_2021_2022['ken'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H-2A dataset without Places API imputed counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2a_df = pd.read_parquet(\"../binaries/h2a_raw.parquet\")\n",
    "\n",
    "# Years prior to 2008 do not have worksite location information\n",
    "h2a_df = h2a_df[h2a_df['FiscalYear'].astype(int) >= 2008]\n",
    "h2a_df['ken'] = False\n",
    "\n",
    "h2a_df = pd.concat([h2a_df, df_2021_2022])\n",
    "\n",
    "# Remove non-US locations\n",
    "h2a_df = h2a_df[h2a_df['WORKSITE_STATE'] != 'AB'].copy() # AB is Alberta, Canada\n",
    "h2a_df = h2a_df[h2a_df['WORKSITE_STATE'] != 'SK'].copy() # SK is Saskatchewan, Canada\n",
    "h2a_df = h2a_df[h2a_df['WORKSITE_STATE'] != 'MB'].copy() # MB is Manitoba, Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain list of states and counties in H-2A data\n",
    "state_county_df = h2a_df[['WORKSITE_STATE', 'WORKSITE_COUNTY']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column with fixed names for counties\n",
    "state_county_df['state'] = state_county_df['WORKSITE_STATE']\n",
    "state_county_df['county'] = state_county_df['WORKSITE_COUNTY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix county names with multiple counties, or with city names\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'HARRISON & BOURBON COUNTIES', 'county'] = 'HARRISON,BOURBON'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'STAMPING GROUND SCOTT COUNTY', 'county'] = 'SCOTT'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'HENRY & UNION COUNTY', 'county'] = 'HENRY,UNION'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'SILVER BOW AND MADISON', 'county'] = 'SILVER BOW,MADISON'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'CANDLER BULLOCH & EVANS COUNTIES', 'county'] = 'CANDLER,BULLOCH,EVANS'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'SAMPSON AND JOHNSTON COUNTIES', 'county'] = 'SAMPSOM,JOHNSTON'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'LINCOLN AND BAYFIELD COUNTIES', 'county'] = 'LINCOLN,BAYFIELD'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'ALLEN & MONROE COUNTIES', 'county'] = 'ALLEN,MONROE'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'OWEN & HENRY COUNTIES', 'county'] = 'OWEN,HENRY'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'UPTON/MAGNOLIA HART COUNTY', 'county'] = 'UPTON,MAGNOLIA,HART'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'WARREN & SIMPSON COUNTY', 'county'] = 'WARREN,SIMPSON'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'CASEY & BOYLE COUNTIES', 'county'] = 'CASEY,BOYLE'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'MONTGOMERY & HALIFAX COUNTIES', 'county'] = 'MONTOGOMERY,HALIFAX'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'COUNTIES: RISING SUN DILLISBORO DEARBORN', 'county'] = 'RISING SUN,DILLISBORO,DEARBORN'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'GEORGE AND RANKIN COUNTIES', 'county'] = 'GEORGE,RANKIN'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'MISSAUKEE WEXFORD OSCEOLA & ANTRIM COUNTIES', 'county'] = 'MISSAUKEE,WEXFORD,OSCEOLA,ANTRIM'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'WASHAKIE BIG HORN JOHNSON COUNTY', 'county'] = 'WASHAKIE,BIG HORN,JOHNSON'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'SOMERSET/BALD MOUNTAIN-UNORGANIZED TS', 'county'] = 'SOMERSET,BALD MOUNTAIN'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'MAHLEUR AND HARNEY COUNTIES', 'county'] = 'MAHLEUR,HARNEY'\n",
    "state_county_df.loc[state_county_df['WORKSITE_COUNTY'] == 'FRANKLIN & SOMERSET', 'county'] = 'FRANKLIN,SOMERSET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create duplicate entries for applications with multiple counties that are separated with a comma\n",
    "state_county_df['county_list'] = state_county_df['county'].str.split(',')\n",
    "state_county_df = state_county_df.explode('county_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop entries with no county or no state\n",
    "state_county_df = state_county_df[(state_county_df['county_list'].notna()) & (state_county_df['state'].notna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use addfips package to get FIPS codes for each state-county pair\n",
    "af = addfips.AddFIPS()\n",
    "state_county_df['fips_nonapi'] = state_county_df.apply(lambda x: af.get_county_fips(x['county_list'], state=x['state']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse back into WORKSITE_STATE and WORKSITE_COUNTY\n",
    "# Use transform as otherwise pandas freaks out when collapsed df has fewer rows than original df, producing all NaNs\n",
    "state_county_df['fips_nonapi'] = state_county_df['fips_nonapi'].astype('string').fillna('')\n",
    "state_county_df['fips_nonapi'] = state_county_df.groupby(['WORKSITE_STATE', 'WORKSITE_COUNTY'])['fips_nonapi'].transform(lambda x: ','.join(x))\n",
    "state_county_df.drop_duplicates(subset=['WORKSITE_STATE', 'WORKSITE_COUNTY', 'fips_nonapi'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add non-API county FIPS codes to entries\n",
    "h2a_nonapi_df = h2a_df.merge(state_county_df, left_on=['WORKSITE_STATE', 'WORKSITE_COUNTY'], right_on=['WORKSITE_STATE', 'WORKSITE_COUNTY'], how='left')\n",
    "h2a_nonapi_df = h2a_nonapi_df[h2a_nonapi_df['ken'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2a_2021_2022_nonapi_df = df_2021_2022.merge(state_county_df, left_on=['WORKSITE_STATE', 'WORKSITE_COUNTY'], right_on=['WORKSITE_STATE', 'WORKSITE_COUNTY'], how='left')\n",
    "# Save as binary to be processed in R\n",
    "h2a_2021_2022_nonapi_df.to_parquet(\"../binaries/h2a_2021_2022_with_fips.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Places API to impute counties for unmatched entries; put in separate parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export list of unmatched state-county pairs\n",
    "unmatched_counties_df = state_county_df[state_county_df['fips_nonapi'] == '']\n",
    "\n",
    "unmatched_counties_df = unmatched_counties_df.assign(place_name = unmatched_counties_df['county_list'] + ', ' + unmatched_counties_df['state'].astype(str))\n",
    "place_names_list = list(unmatched_counties_df['place_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Google Places API to fill in missing county names if possible\n",
    "# Google maps API key from my account\n",
    "# Import API key stored in text file\n",
    "with open(\"../tools/google_places_api_key.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "api_key = lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make API call for each place name to obtain Google Places placeID\n",
    "# Base url to call findplace API\n",
    "# base_url = \"https://maps.googleapis.com/maps/api/place/findplacefromtext/json?\"\n",
    "# placeid_dict = {}\n",
    "# for place_ind in range(0, len(place_names_list)):\n",
    "    \n",
    "#     place_name = place_names_list[place_ind]\n",
    "#     print('Currently looking up ' + place_name)\n",
    "\n",
    "#     # Create API request\n",
    "#     # URL'ed location name we want to search\n",
    "#     input = urllib.parse.quote(place_name) # Encode place name as URL string\n",
    "#     request_url = base_url + \"input=\" + input + \"&inputtype=textquery\" + \"&key=\" + api_key\n",
    "\n",
    "#     payload = {}\n",
    "#     headers = {}\n",
    "\n",
    "#     # Sleep one second between each API call\n",
    "#     time.sleep(1)\n",
    "\n",
    "#     # Make API call\n",
    "#     response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#     response_json = response.json()\n",
    "    \n",
    "#     # If API call is successful, then place response result into dict\n",
    "#     if response_json['status']=='OK':\n",
    "#         print('Successful')\n",
    "#         placeid_dict[place_name] = response_json\n",
    "#     else:\n",
    "#         # If API call is unsuccessful, then wait 5 seconds and retry\n",
    "#         print('NOT successful, retrying')\n",
    "#         time.sleep(5)\n",
    "#         response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#         response_json = response.json()\n",
    "\n",
    "#         if response_json['status']=='OK':\n",
    "#             print('Retry successful')\n",
    "#             placeid_dict[place_name] = response_json\n",
    "#         else:\n",
    "#             error_type = response_json['status']\n",
    "#             print('Retry unsuccessful, error: ' + error_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save API request results as JSON\n",
    "# with open('json/placeid_api_request_result.json', 'w') as f:\n",
    "#     json.dump(placeid_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON of API responses and put into DataFrame\n",
    "with open('json/placeid_api_request_result.json', 'r') as infile:\n",
    "    placeid_dict = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put place IDs into DataFrame\n",
    "placeid_df = pd.DataFrame(columns=['name_to_search', 'placeid'])\n",
    "\n",
    "for search_term, response in placeid_dict.items():\n",
    "    number_of_candidates = len(response['candidates'])\n",
    "    for response_ind in range(0, number_of_candidates):\n",
    "        address = response['candidates'][response_ind]['place_id']\n",
    "        placeid_df.loc[len(placeid_df)] = [search_term, address]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use Place details API to get county names\n",
    "# # base_url = 'https://maps.googleapis.com/maps/api/place/details/json?'\n",
    "# place_details_dict = {}\n",
    "\n",
    "# # Iterate over each place ID\n",
    "# for index, row in placeid_df.iterrows():\n",
    "#     print(row['name_to_search'], row['placeid'])\n",
    "\n",
    "#     # Create API request\n",
    "#     input = row['placeid']\n",
    "#     place_name = row['name_to_search']\n",
    "#     request_url = base_url + \"placeid=\" + input + \"&key=\" + api_key\n",
    "\n",
    "#     payload = {}\n",
    "#     headers = {}\n",
    "\n",
    "#     response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#     response_json = response.json()\n",
    "\n",
    "#     # If API call is successful, then place response result into dict\n",
    "#     if response_json['status']=='OK':\n",
    "#         print('Successful')\n",
    "#         place_details_dict[input] = response_json\n",
    "#     else:\n",
    "#         # If API call is unsuccessful, then wait 5 seconds and retry\n",
    "#         print('NOT successful, retrying')\n",
    "#         time.sleep(5)\n",
    "#         response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#         response_json = response.json()\n",
    "\n",
    "#         if response_json['status']=='OK':\n",
    "#             print('Retry successful')\n",
    "#             place_details_dict[input] = response_json\n",
    "#         else:\n",
    "#             error_type = response_json['status']\n",
    "#             print('Retry unsuccessful, error: ' + error_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save API request results as JSON\n",
    "# with open('json/place_details_api_request_result.json', 'w') as f:\n",
    "#     json.dump(place_details_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON of API responses and put into DataFrame\n",
    "with open('json/place_details_api_request_result.json', 'r') as infile:\n",
    "    place_details_dict = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store county name from place details into dictionary (store state names too as there may be incorrect states)\n",
    "county_name_dict = {}\n",
    "state_name_dict = {}\n",
    "\n",
    "for placeid, place_ind in place_details_dict.items():\n",
    "    address_components_dict_list = place_details_dict[placeid]['result']['address_components']\n",
    "    for component_ind in range(len(address_components_dict_list)):\n",
    "        # Address components administrative level\n",
    "        component_admin_level = address_components_dict_list[component_ind]['types'][0]\n",
    "        \n",
    "        # County name\n",
    "        if component_admin_level == 'administrative_area_level_2':\n",
    "            county_name_dict[placeid] = address_components_dict_list[component_ind]['long_name']\n",
    "        # State name\n",
    "        elif component_admin_level == 'administrative_area_level_1':\n",
    "            state_name_dict[placeid] = address_components_dict_list[component_ind]['long_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert state and county name dicts into DataFrames\n",
    "placeid_state_df = pd.DataFrame.from_dict(state_name_dict, orient='index', columns=['state'])\n",
    "placeid_state_df.reset_index(inplace=True)\n",
    "placeid_state_df.rename(columns={'index': 'placeid'}, inplace=True)\n",
    "\n",
    "placeid_county_df = pd.DataFrame.from_dict(county_name_dict, orient='index', columns=['county'])\n",
    "placeid_county_df.reset_index(inplace=True)\n",
    "placeid_county_df.rename(columns={'index':'placeid'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add state and county names back into placeid_df\n",
    "placeid_df = placeid_df.merge(placeid_state_df, left_on='placeid', right_on='placeid', how='left')\n",
    "placeid_df = placeid_df.merge(placeid_county_df, left_on='placeid', right_on='placeid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix a few incorrect entries\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'WASHINGOTN, ME', 'county'] = 'Washington County'\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'FREDRICK, VA', 'county'] = 'Frederick County'\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'UPTON, KY', 'county'] = 'Hardin County' # Upton is mostly located in Hardin county\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'CA, CA', 'county'] = np.nan\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'MOFFAT, WY', 'county'] = 'Moffat County'\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'MOFFAT, WY', 'state'] = 'CO' # Moffat County is on the WY/CO border in CO\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'MADERA, ID', 'county'] = np.nan # Not sure what Madera, ID is\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'MADERA, ID', 'state'] = np.nan\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'GOODING, CO', 'county'] = 'Boulder County' # Gooding is in Boulder County, CO\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'GOODING, CO', 'state'] = 'CO'\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'CUSTER, ND', 'county'] = np.nan # Not sure what CUSTER, ND refers to\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'CUSTER, ND', 'state'] = np.nan\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'ORANGE, OK', 'county'] = np.nan # Not sure what ORANGE, OK refers to\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'ORANGE, OK', 'state'] = np.nan\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'ALACHUA, GA', 'county'] = 'Alachua County' # Alachua County is in Florida\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'ALACHUA, GA', 'state'] = 'Florida'\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'MOFFAT, FL', 'county'] = np.nan # There is no place called Moffat in Florida\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'MOFFAT, FL', 'state'] = np.nan\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'JONES, SC', 'county'] = np.nan # There is no Jones in SC\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'JONES, SC', 'state'] = np.nan\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'DUNN, SD', 'county'] = np.nan # There is no Dunn in SD\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'DUNN, SD', 'state'] = np.nan\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'DICKEY, SD', 'county'] = np.nan # There is no Dickey in SD\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'DICKEY, SD', 'state'] = np.nan\n",
    "\n",
    "# Western Connecticut consists of Lichtfield and Fairfield counties\n",
    "connecticut_row = placeid_df[placeid_df['name_to_search'] == 'WESTERN CONNECTICUT, CT'].copy()\n",
    "connecticut_row['county'] = 'Litchfield County'\n",
    "placeid_df = pd.concat([placeid_df, connecticut_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop locations that do not have a county name (including those incorrectly resolved that I removed)\n",
    "placeid_df.dropna(subset=['county'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FIPS code\n",
    "placeid_df['fips'] = placeid_df.apply(lambda x: af.get_county_fips(x['county'], state=x['state']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LaSalle Parish in Louisiana is not in the addfips package(?), so add it manually\n",
    "placeid_df.loc[placeid_df['name_to_search'] == 'LASALLE, LA', 'fips'] = '22059'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only place names and FIPS codes, then collapse to unique place names\n",
    "placeid_df = placeid_df[['name_to_search', 'fips']]\n",
    "placeid_df = placeid_df.groupby(['name_to_search']).agg({'fips': lambda x: x.tolist()}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to append back into original DataFrame of states and counties\n",
    "placeid_df.rename(columns={'name_to_search': 'place_name'}, inplace=True)\n",
    "unmatched_counties_df.drop(columns=['fips_nonapi'], inplace=True)\n",
    "api_counties_df = unmatched_counties_df.merge(placeid_df, left_on='place_name', right_on='place_name', how='left')\n",
    "api_counties_df.drop(columns=['place_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some final cleaning\n",
    "api_counties_df.loc[api_counties_df['WORKSITE_COUNTY'] == 'KNOK', 'fips'] = '23027' # Knox town in Maine was misspelled as KNOK\n",
    "api_counties_df.loc[api_counties_df['WORKSITE_COUNTY'] == 'VARIOUS COUNTIES IN WESTERN', 'fips'] = np.nan # Various counties in NC not specified\n",
    "api_counties_df.loc[api_counties_df['WORKSITE_COUNTY'] == 'VARIOUS COUNTIES', 'fips'] = np.nan # Various counties in NC not specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert multi-FIPS entries from list to string\n",
    "api_counties_df = api_counties_df[api_counties_df['fips'].notna()]\n",
    "api_counties_df['fips_string'] = api_counties_df['fips'].apply(lambda x: ','.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_counties_df.loc[api_counties_df['WORKSITE_COUNTY'] == 'KNOK', 'fips_string'] = '23027' # Somehow FIPS cannot be stored as a list, so join splits the string; this fixes it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to merge when all FIPS codes are stored as strings only\n",
    "api_counties_df.drop(columns=['fips'], inplace=True)\n",
    "api_counties_df['fips_string'] = api_counties_df['fips_string'].astype('string').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all FIPS codes for each WORKSITE_STATE and WORKSITE_COUNTY\n",
    "merged_df = state_county_df.merge(api_counties_df, left_on=['WORKSITE_STATE', 'WORKSITE_COUNTY', 'state', 'county', 'county_list'], right_on=['WORKSITE_STATE', 'WORKSITE_COUNTY',  'state', 'county', 'county_list'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine FIPS codes from both sources\n",
    "merged_df['fips_api'] = merged_df['fips_nonapi'].where(merged_df['fips_string'].isna(), merged_df['fips_string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse back into unique WORKSITE_STATE and WORKSITE_COUNTY pairs\n",
    "merged_df = merged_df[['WORKSITE_STATE', 'WORKSITE_COUNTY', 'fips_api']]\n",
    "merged_df.loc[:, 'fips_api'] = merged_df['fips_api'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use transform as otherwise pandas freaks out when collapsed df has fewer rows than original df, producing all NaNs\n",
    "merged_df['fips_api'] = merged_df.groupby(['WORKSITE_STATE', 'WORKSITE_COUNTY'])['fips_api'].transform(lambda x: ','.join(x))\n",
    "merged_df.drop_duplicates(subset=['WORKSITE_STATE', 'WORKSITE_COUNTY', 'fips_api'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge back into H-2A data\n",
    "h2a_merged_df = h2a_nonapi_df.merge(merged_df, left_on=['WORKSITE_STATE', 'WORKSITE_COUNTY'], right_on=['WORKSITE_STATE', 'WORKSITE_COUNTY'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the remaining entries, we will try to get counties from the WORKSITE_CITY names\n",
    "state_city_df = h2a_merged_df[h2a_merged_df['fips_api'].isna()][['WORKSITE_STATE', 'WORKSITE_CITY']].dropna().drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# United States of America Python Dictionary to translate States,\n",
    "# Districts & Territories to Two-Letter codes and vice versa.\n",
    "#\n",
    "# Canonical URL: https://gist.github.com/rogerallen/1583593\n",
    "#\n",
    "# Dedicated to the public domain.  To the extent possible under law,\n",
    "# Roger Allen has waived all copyright and related or neighboring\n",
    "# rights to this code.  Data originally from Wikipedia at the url:\n",
    "# https://en.wikipedia.org/wiki/ISO_3166-2:US\n",
    "#\n",
    "# Automatically Generated 2021-09-11 18:04:36 via Jupyter Notebook from\n",
    "# https://gist.github.com/rogerallen/d75440e8e5ea4762374dfd5c1ddf84e0 \n",
    "\n",
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\"\n",
    "}\n",
    "# invert the dictionary\n",
    "abbrev_to_us_state = dict(map(reversed, us_state_to_abbrev.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use state name instead of abbreviation; suspect that Google Places API is more accurate with state names\n",
    "state_city_df['state_name'] = state_city_df['WORKSITE_STATE'].map(abbrev_to_us_state)\n",
    "state_city_df['city_state_name'] = state_city_df['WORKSITE_CITY'] + ', ' + state_city_df['state_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split API search into chunks of 100\n",
    "# # Base url to call findplace API\n",
    "# base_url = \"https://maps.googleapis.com/maps/api/place/findplacefromtext/json?\"\n",
    "\n",
    "# for x in range(1, 17):\n",
    "#     x1 = x*100\n",
    "#     x2 = (x+1)*100\n",
    "#     df = state_city_df.iloc[x1:x2]\n",
    "\n",
    "#     # Make API call for each place name to obtain Google Places placeID\n",
    "#     city_placeid_dict = {}\n",
    "\n",
    "#     for ind, row in df.iterrows():\n",
    "#         place_name = row['city_state_name']\n",
    "#         print('Currently looking up ' + place_name)\n",
    "\n",
    "#         # Create API request\n",
    "#         # URL'ed location name we want to search\n",
    "#         input = urllib.parse.quote(place_name) # Encode place name as URL string\n",
    "#         request_url = base_url + \"input=\" + input + \"&inputtype=textquery\" + \"&key=\" + api_key\n",
    "\n",
    "#         payload = {}\n",
    "#         headers = {}\n",
    "\n",
    "#         # Sleep one second between each API call\n",
    "#         time.sleep(1)\n",
    "\n",
    "#         # Make API call\n",
    "#         response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#         response_json = response.json()\n",
    "        \n",
    "#         # If API call is successful, then place response result into dict\n",
    "#         if response_json['status']=='OK':\n",
    "#             print('Successful')\n",
    "#             city_placeid_dict[place_name] = response_json\n",
    "#         else:\n",
    "#             # If API call is unsuccessful, then wait 5 seconds and retry\n",
    "#             print('NOT successful, retrying')\n",
    "#             time.sleep(5)\n",
    "#             response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#             response_json = response.json()\n",
    "\n",
    "#             if response_json['status']=='OK':\n",
    "#                 print('Retry successful')\n",
    "#                 city_placeid_dict[place_name] = response_json\n",
    "#             else:\n",
    "#                 error_type = response_json['status']\n",
    "#                 print('Retry unsuccessful, error: ' + error_type)\n",
    "\n",
    "#     # Save API request results as JSON\n",
    "#     with open(f'json/city_placeid_api_request_result_{x}.json', 'w') as f:\n",
    "#         json.dump(city_placeid_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON of API responses containing place IDs and put into dictionary\n",
    "city_placeid_dict = {}\n",
    "for x in range(0, 17):\n",
    "    with open(f'json/city_placeid_api_request_result_{x}.json', 'r') as infile:\n",
    "        city_placeid_dict[x] = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put place IDs into DataFrame\n",
    "placeid_df = pd.DataFrame(columns=['city_state_name', 'placeid'])\n",
    "\n",
    "for x in range(0, 17):\n",
    "    for search_term, response in city_placeid_dict[x].items():\n",
    "        number_of_candidates = len(response['candidates'])\n",
    "        for response_ind in range(0, number_of_candidates):\n",
    "            address = response['candidates'][response_ind]['place_id']\n",
    "            placeid_df.loc[len(placeid_df)] = [search_term, address]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only results with no uncertainty in location\n",
    "placeid_df_no_duplicates = placeid_df.drop_duplicates(subset=['city_state_name'], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now split up API requests into chunks of 100\n",
    "# placeid_df_no_duplicates.reset_index(inplace=True, drop=True)\n",
    "# # Base url to call Place details API\n",
    "# base_url = 'https://maps.googleapis.com/maps/api/place/details/json?'\n",
    "\n",
    "# for x in range(1, 16):\n",
    "#     x1 = x*100\n",
    "#     x2 = (x+1)*100\n",
    "#     df = placeid_df_no_duplicates.iloc[x1:x2]\n",
    "\n",
    "#     # Use Place details API to get county names\n",
    "#     place_details_dict = {}\n",
    "\n",
    "#     # Iterate over each place ID\n",
    "#     for index, row in df.iterrows():\n",
    "#         print(row['city_state_name'], row['placeid'])\n",
    "\n",
    "#         # Create API request\n",
    "#         input = row['placeid']\n",
    "#         place_name = row['city_state_name']\n",
    "#         request_url = base_url + \"placeid=\" + input + \"&key=\" + api_key\n",
    "\n",
    "#         payload = {}\n",
    "#         headers = {}\n",
    "\n",
    "#         response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#         response_json = response.json()\n",
    "\n",
    "#         # If API call is successful, then place response result into dict\n",
    "#         if response_json['status']=='OK':\n",
    "#             print('Successful')\n",
    "#             place_details_dict[input] = response_json\n",
    "#         else:\n",
    "#             # If API call is unsuccessful, then wait 5 seconds and retry\n",
    "#             print('NOT successful, retrying')\n",
    "#             time.sleep(5)\n",
    "#             response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#             response_json = response.json()\n",
    "\n",
    "#             if response_json['status']=='OK':\n",
    "#                 print('Retry successful')\n",
    "#                 place_details_dict[input] = response_json\n",
    "#             else:\n",
    "#                 error_type = response_json['status']\n",
    "#                 print('Retry unsuccessful, error: ' + error_type)\n",
    "\n",
    "#     # Save API request results as JSON\n",
    "#     with open(f'json/city_place_details_api_request_result_{x}.json', 'w') as f:\n",
    "#         json.dump(place_details_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store county name from place details into dictionary (store state names too as there may be incorrect states)\n",
    "county_name_dict = {}\n",
    "state_name_dict = {}\n",
    "\n",
    "for x in range(0, 16):\n",
    "    # Load JSON of API responses and put into DataFrame\n",
    "    with open(f'json/city_place_details_api_request_result_{x}.json', 'r') as infile:\n",
    "        place_details_dict = json.load(infile)\n",
    "\n",
    "    for placeid, place_ind in place_details_dict.items():\n",
    "        address_components_dict_list = place_details_dict[placeid]['result']['address_components']\n",
    "        for component_ind in range(len(address_components_dict_list)):\n",
    "            # Address components administrative level\n",
    "            component_admin_level = address_components_dict_list[component_ind]['types'][0]\n",
    "            \n",
    "            # County name\n",
    "            if component_admin_level == 'administrative_area_level_2':\n",
    "                county_name_dict[placeid] = address_components_dict_list[component_ind]['long_name']\n",
    "            # State name\n",
    "            elif component_admin_level == 'administrative_area_level_1':\n",
    "                state_name_dict[placeid] = address_components_dict_list[component_ind]['long_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert state and county name dicts into DataFrames\n",
    "placeid_state_df = pd.DataFrame.from_dict(state_name_dict, orient='index', columns=['state'])\n",
    "placeid_state_df.reset_index(inplace=True)\n",
    "placeid_state_df.rename(columns={'index': 'placeid'}, inplace=True)\n",
    "\n",
    "placeid_county_df = pd.DataFrame.from_dict(county_name_dict, orient='index', columns=['county'])\n",
    "placeid_county_df.reset_index(inplace=True)\n",
    "placeid_county_df.rename(columns={'index':'placeid'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add state and county names back into placeid_df\n",
    "placeid_df_no_duplicates = placeid_df_no_duplicates.merge(placeid_state_df, left_on='placeid', right_on='placeid', how='left')\n",
    "placeid_df_no_duplicates = placeid_df_no_duplicates.merge(placeid_county_df, left_on='placeid', right_on='placeid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix a few incorrect entries\n",
    "placeid_df_no_duplicates.loc[placeid_df_no_duplicates['city_state_name'] == 'SEE BELOW, New York', 'county'] = np.nan\n",
    "\n",
    "# Drop locations that do not have a county name (including those incorrectly resolved that I removed)\n",
    "placeid_df_no_duplicates.dropna(subset=['state', 'county'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FIPS code\n",
    "placeid_df_no_duplicates['fips_api_city'] = placeid_df_no_duplicates.apply(lambda x: af.get_county_fips(x['county'], state=x['state']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge FIPS code back to original DataFrame\n",
    "placeid_df_no_duplicates.loc[:, 'fips_api_city'] = placeid_df_no_duplicates['fips_api_city'].astype('string')\n",
    "placeid_df_no_duplicates = placeid_df_no_duplicates[['city_state_name', 'fips_api_city']]\n",
    "state_city_df = state_city_df.merge(placeid_df_no_duplicates, left_on='city_state_name', right_on='city_state_name', how='left')\n",
    "state_city_df.drop(columns=['state_name', 'city_state_name'], inplace=True)\n",
    "\n",
    "h2a_merged_df = h2a_merged_df.merge(state_city_df, left_on=['WORKSITE_STATE', 'WORKSITE_CITY'], right_on=['WORKSITE_STATE', 'WORKSITE_CITY'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create finalized combined FIPS column\n",
    "h2a_merged_df['fips_api'] = h2a_merged_df['fips_api'].astype('string').fillna('')\n",
    "h2a_merged_df['fips_api_city'] = h2a_merged_df['fips_api_city'].astype('string').fillna('')\n",
    "h2a_merged_df.loc[h2a_merged_df['fips_api']=='', 'fips_api'] = h2a_merged_df['fips_api_city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as binary to be processed in R\n",
    "h2a_merged_df.to_parquet(\"../binaries/h2a_with_fips.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

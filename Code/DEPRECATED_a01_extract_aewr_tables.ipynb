{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import pdfplumber\n",
    "import re\n",
    "import us\n",
    "import marimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CFR documents matching search criterion\n",
    "cfr_csv = pd.read_csv(\"../Data/aewr/documents_matching_labor_certification_process_adverse_effect_wage_rates_from_labor_department_and_of_type_notice.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save all Federal Register PDFs matching search criterion\n",
    "# Path(\"../Data/aewr/pdf/\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# for index, row in cfr_csv.iterrows():\n",
    "#     document_number = row['document_number'].strip()\n",
    "#     url_string = row['pdf_url']\n",
    "#     response = requests.get(url_string) # URL of the CFR document in pdf format\n",
    "#     pdf_file = Path(f\"../Data/aewr/pdf/{document_number}.pdf\")\n",
    "#     pdf_file.write_bytes(response.content)\n",
    "#     time.sleep(1) # Pause for 1 second between each pdf download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to extract text from pdf by column\n",
    "x0 = 0.05  # Distance of left side of column 1 from left side of page.\n",
    "x1 = 0.35  # Distance of right side of column 1 from left side of page\n",
    "x2 = 0.65  # Distance of right side of column 2 from left side of page\n",
    "x3 = 0.95  # Distance of right side of column 3 from left side of page\n",
    "\n",
    "y0 = 0.05  # Distance of top from top of page.\n",
    "y1 = 0.95  # Distance of bottom from top of page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 states minus Alaska\n",
    "state_names = [\"Alabama\", \"Arkansas\", \"Arizona\", \"California\", \n",
    "\"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \n",
    "\"Hawaii\", \"Iowa\", \"Idaho\", \"Illinois\", \"Indiana\", \n",
    "\"Kansas\", \"Kentucky\", \"Louisiana\", \"Massachusetts\", \"Maryland\", \n",
    "\"Maine\", \"Michigan\", \"Minnesota\", \"Missouri\", \"Mississippi\", \n",
    "\"Montana\", \"North Carolina\", \"North Dakota\", \"Nebraska\", \"New Hampshire\", \n",
    "\"New Jersey\", \"New Mexico\", \"Nevada\", \"New York\", \"Ohio\", \n",
    "\"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \n",
    "\"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Virginia\", \n",
    "\"Vermont\", \"Washington\", \"Wisconsin\", \"West Virginia\", \"Wyoming\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file 00-2547, for year 2000.\n",
      "Parsing file 01-19298, for year 2001.\n",
      "Parsing file 02-12376, for year 2002.\n",
      "Parsing file 03-4500, for year 2003.\n",
      "Parsing file 04-4731, for year 2004.\n",
      "Parsing file 2010-3078, for year 2010.\n",
      "Parsing file 2011-32842, for year 2012.\n",
      "Parsing file 2011-4419, for year 2011.\n",
      "Parsing file 2013-00117, for year 2013.\n",
      "Parsing file 2013-31555, for year 2014.\n",
      "Parsing file 2014-29746, for year 2015.\n",
      "Parsing file 2015-32114, for year 2016.\n",
      "Parsing file 2016-30928, for year 2017.\n",
      "Parsing file 2017-27528, for year 2018.\n",
      "Parsing file 2018-28030, for year 2019.\n",
      "Parsing file 2019-27410, for year 2020.\n",
      "Parsing file 2021-03752, for year 2021.\n",
      "Parsing file 2021-27119, for year 2022.\n",
      "Parsing file 2022-27332, for year 2023.\n",
      "Parsing file 2023-27435, for year 2024.\n",
      "Parsing file 95-2964, for year 1995.\n",
      "Parsing file 96-2714, for year 1996.\n",
      "Parsing file 97-3095, for year 1997.\n",
      "Parsing file 98-4051, for year 1998.\n",
      "Parsing file 99-3269, for year 1999.\n",
      "Parsing file E5-824, for year 2005.\n",
      "Parsing file E6-3841, for year 2006.\n",
      "Parsing file E7-2859, for year 2007.\n",
      "Parsing file E8-3567, for year 2008.\n",
      "Parsing file E9-12434, for year 2009.\n"
     ]
    }
   ],
   "source": [
    "rows_list = [] # List of rows (as dictionaries) to be converted to dataframe\n",
    "for file in Path(\"../Data/aewr/pdf/\").iterdir():\n",
    "    # Read all pdf files in directory\n",
    "    file_type = file.suffix\n",
    "    if file_type != '.pdf':\n",
    "        continue\n",
    "\n",
    "    file_name = file.stem\n",
    "    \n",
    "    if file_name == '03-6559': # correction issued for 2003 AEWR for Hawaii\n",
    "        continue\n",
    "    elif file_name == '2013-00115': # separate AEWR for open range livestock occupation newly added \n",
    "        continue\n",
    "    elif file_name == '2023-12896': # update to AEWR used for states and territories not covered by the Farm Labor Survey\n",
    "        continue\n",
    "\n",
    "    pdf_content = ''\n",
    "\n",
    "    with pdfplumber.open(file) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            width = page.width\n",
    "            height = page.height\n",
    "\n",
    "            # Crop pages\n",
    "            left_bbox = (x0*float(width), y0*float(height), x1*float(width), y1*float(height))\n",
    "            middle_bbox = (x1*float(width), y0*float(height), x2*float(width), y1*float(height))\n",
    "            right_bbox = (x2*float(width), y0*float(height), x3*float(width), y1*float(height))\n",
    "\n",
    "            page_crop = page.crop(bbox = left_bbox)\n",
    "            left_text = page_crop.extract_text()\n",
    "            page_crop = page.crop(bbox = middle_bbox)\n",
    "            middle_text = page_crop.extract_text()\n",
    "            page_crop = page.crop(bbox = right_bbox)\n",
    "            right_text = page_crop.extract_text()\n",
    "\n",
    "            page_content = '\\n'.join([left_text, middle_text, right_text])\n",
    "            pdf_content = pdf_content + page_content\n",
    "\n",
    "        full_string = pdf_content.replace('\\n', '')\n",
    "\n",
    "        # Year AEWR notice is published for\n",
    "        try:\n",
    "            # Skips files for ranching occupations, which we don't care about\n",
    "            # DOL also decided to change the table titling format for 2025 for whatever reason, but we don't care about 2025 for now\n",
    "            notice_year_string = re.search(r'\\d\\d\\d\\d [Aa][Dd][Vv][Ee][Rr][Ss][Ee] [Ee][Ff][Ff][Ee][Cc][Tt]', full_string).group(0)\n",
    "            notice_year = notice_year_string[0:4]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Parse each PDF\n",
    "        print(f'Parsing file {file_name}, for year {notice_year}.')\n",
    "\n",
    "        # Parse state names and corresponding AEWRs within each PDF using regex\n",
    "        for state in state_names:\n",
    "\n",
    "            if notice_year == '2005' and state == 'Idaho': # typo in the 2005 AEWR table for Idaho\n",
    "                state = 'Idah'\n",
    "            \n",
    "            # This regex captures everything between the state name and the first .dd after, which are the last 2 digits of the AEWR for that state\n",
    "            state_wage = re.search(fr'{state}[ .$\\d]*?\\.\\d\\d', full_string)\n",
    "\n",
    "            # Parse wage within each 'state...wage' string using regex\n",
    "            state_wage_string = state_wage.group(0)\n",
    "            wage = re.search(r'\\d+?\\.\\d\\d', state_wage_string)\n",
    "\n",
    "            # Put results into dataframe\n",
    "            row_dict = {} # define row in dictionary format, column names as keys\n",
    "            aewr = wage.group(0)\n",
    "            \n",
    "            if notice_year == '2005' and state == 'Idah': # fix typo in the 2005 AEWR table for Idaho\n",
    "                state = 'Idaho'\n",
    "            if notice_year == '2003' and state == 'Hawaii': # correction issued for 2003 AEWR for Hawaii\n",
    "                aewr = '9.42'\n",
    "\n",
    "            row_dict.update({'state_name':state, 'year':notice_year, 'aewr':aewr}) \n",
    "            rows_list.append(row_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all rows into a dataframe\n",
    "df = pd.DataFrame(rows_list)\n",
    "\n",
    "# Add state FIPS code and export\n",
    "df['state_fips'] = df['state_name'].map(us.states.mapping('name', 'fips'))\n",
    "df = df.rename(columns={'state_fips':'state_fips_code'})\n",
    "df = df.drop(columns=['state_name'])\n",
    "df.to_csv(\"../Data/aewr/state_year_aewr.csv\", index=False)\n",
    "df.to_parquet(\"../files_for_phil/aewr.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h-2a-paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

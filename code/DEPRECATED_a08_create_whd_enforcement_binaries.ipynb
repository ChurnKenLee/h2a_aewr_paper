{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pandas.api.types import union_categoricals\n",
    "from itertools import islice\n",
    "import re\n",
    "import addfips\n",
    "import requests\n",
    "import urllib\n",
    "import time\n",
    "DC_STATEHOOD = 1 # Enables DC to be included in the state list\n",
    "import us\n",
    "import rapidfuzz\n",
    "from rapidfuzz import fuzz\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Census geographic codes file\n",
    "census_county = pd.read_csv(\"../Data/census_geography_codes/national_county2020.txt\", sep='|', dtype = 'string', keep_default_na=False).apply(lambda x: x.str.upper())\n",
    "census_countysub = pd.read_csv(\"../Data/census_geography_codes/national_cousub2020.txt\", sep='|', dtype = 'string', keep_default_na=False).apply(lambda x: x.str.upper())\n",
    "census_place = pd.read_csv(\"../Data/census_geography_codes/national_place2020.txt\", sep='|', dtype = 'string', keep_default_na=False).apply(lambda x: x.str.upper())\n",
    "census_placebycounty = pd.read_csv(\"../Data/census_geography_codes/national_place_by_county2020.txt\", sep='|', dtype = 'string', keep_default_na=False).apply(lambda x: x.str.upper())\n",
    "census_zip = pd.read_csv(\"../Data/census_geography_codes/tab20_zcta520_county20_natl.txt\", sep='|', dtype = 'string', keep_default_na=False).apply(lambda x: x.str.upper())\n",
    "\n",
    "# Add FIPS column\n",
    "census_county['fips'] = census_county['STATEFP'] + census_county['COUNTYFP']\n",
    "census_placebycounty['fips'] = census_placebycounty['STATEFP'] + census_placebycounty['COUNTYFP']\n",
    "census_zip['fips'] = census_zip['GEOID_COUNTY_20']\n",
    "\n",
    "# There may be places and ZIP codes that map to multiple counties; collapse these into unique entries\n",
    "census_county = census_county[['STATE', 'COUNTYNAME', 'fips']]\n",
    "census_place_agg = census_placebycounty.groupby(['STATE', 'COUNTYNAME', 'PLACENAME']).agg({'fips':lambda x: \",\".join(x)}).reset_index()\n",
    "census_zip_agg = census_zip.groupby(['GEOID_ZCTA5_20']).agg({'fips':lambda x: \",\".join(x)}).reset_index()\n",
    "\n",
    "# Drop empty entries\n",
    "census_county = census_county[census_county['COUNTYNAME'] != '']\n",
    "census_place_agg = census_place_agg[census_place_agg['PLACENAME'] != '']\n",
    "census_zip_agg = census_zip_agg[census_zip_agg['GEOID_ZCTA5_20'] != '']\n",
    "census_zip_agg = census_zip_agg.rename(columns = {'GEOID_ZCTA5_20':'zip', 'fips':'fips_from_zip'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns from DOL WHD enforcement data we want\n",
    "whd_dtype_dict = {\n",
    "    'case_id': 'string',\n",
    "    'trade_nm': 'string',\n",
    "    'legal_name': 'string',\n",
    "    'street_addr_1_txt': 'string',\n",
    "    'cty_nm': 'string',\n",
    "    'st_cd': 'string',\n",
    "    'zip_cd': 'string',\n",
    "    'naic_cd': 'string',\n",
    "    'findings_start_date': 'string',\n",
    "    'findings_end_date': 'string',\n",
    "    'h2a_violtn_cnt': 'float',\n",
    "    'h2a_bw_atp_amt': 'float',\n",
    "    'h2a_ee_atp_cnt': 'float',\n",
    "    'h2a_cmp_assd_amt': 'float',\n",
    "    'mspa_violtn_cnt': 'float',\n",
    "    'mspa_bw_atp_amt': 'float',\n",
    "    'mspa_ee_atp_cnt': 'float',\n",
    "    'mspa_cmp_assd_amt': 'float'\n",
    "}\n",
    "\n",
    "whd_cols_list = list(whd_dtype_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOL WHD enforcement data\n",
    "whd_df = pd.read_csv(\"../Data/whd_enforcement/whd_whisard.csv\", usecols = whd_cols_list, dtype = whd_dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only care about H-2A violations\n",
    "whd_df = whd_df[whd_df['h2a_violtn_cnt'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to add county FIPS codes to each entry\n",
    "# We have city names and ZIP codes\n",
    "# Get list of city names, states, and ZIP codes\n",
    "whd_df['city'] = whd_df['cty_nm'].str.upper()\n",
    "whd_df['zip'] = whd_df['zip_cd']\n",
    "whd_df['state'] = whd_df['st_cd']\n",
    "\n",
    "worksite_df = whd_df[['city', 'zip', 'state']]\n",
    "worksite_df = worksite_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up ZIP codes\n",
    "# There are some entries with ZIP codes shorter than 5; pad with zeros\n",
    "worksite_df.loc[((worksite_df['zip'].str.len() < 5) & (worksite_df['zip'] != '')), 'zip'] = worksite_df.loc[((worksite_df['zip'].str.len() < 5) & (worksite_df['zip'] != '')), 'zip'].str.pad(width = 5, side = 'left', fillchar = '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for fuzzy string matching\n",
    "def fuzz_search(census_df, census_col, state_to_search, name_to_match):\n",
    "\n",
    "    def fuzz_match(x, y):\n",
    "        return rapidfuzz.fuzz.WRatio(x, y)\n",
    "    \n",
    "    state_df = census_df[census_df['STATE'] == state_to_search].copy()\n",
    "    state_df['score'] = state_df[census_col].apply(lambda x: fuzz_match(x, name_to_match))\n",
    "    \n",
    "    state_df = state_df.sort_values('score')\n",
    "    \n",
    "    max_score_row = state_df[state_df['score'] == state_df['score'].max()].reset_index()\n",
    "\n",
    "    # Best match\n",
    "    if len(max_score_row) >= 1:\n",
    "        fips = str(max_score_row['fips'][0])\n",
    "        score = str(max_score_row['score'][0])\n",
    "        census_name = (max_score_row[census_col][0])\n",
    "        return(fips, score, census_name)\n",
    "    else:\n",
    "        return('', '', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get matches and match score\n",
    "census_df = census_placebycounty\n",
    "census_col = 'PLACENAME'\n",
    "fuzzy_result_df = worksite_df.apply(lambda x: fuzz_search(census_df, census_col, x.state, x.city), axis=1, result_type='expand')\n",
    "fuzzy_result_df = fuzzy_result_df.rename(columns = {0:'fips_from_city', 1:'score_from_city', 2:'census_name_city'})\n",
    "fuzzy_result_df['score_from_city'] = pd.to_numeric(fuzzy_result_df['score_from_city'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check quality of matches, and define cutoff score\n",
    "match_quality = worksite_df.merge(fuzzy_result_df, left_index=True, right_index=True)\n",
    "match_quality.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 86 appears to be a good cutoff score\n",
    "fuzzy_result_df.loc[fuzzy_result_df['score_from_city'] < 86, ['fips_from_city']] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine matched cities back in\n",
    "worksite_df = worksite_df.merge(fuzzy_result_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FIPS from matched ZIP codes as well\n",
    "worksite_df = worksite_df.merge(census_zip_agg, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined FIPS, and use Google Places API for the rest\n",
    "# Write function with logic for choosing FIPS\n",
    "def fips_choice(county_fips, zip_fips, city_fips):\n",
    "    if (zip_fips != ''):\n",
    "        return(zip_fips)\n",
    "    \n",
    "    if (city_fips != ''):\n",
    "        return(city_fips)\n",
    "    \n",
    "    if (county_fips != ''):\n",
    "        return(county_fips)\n",
    "    \n",
    "    else:\n",
    "        return('')\n",
    "    \n",
    "worksite_df = worksite_df.fillna('')\n",
    "worksite_df['fips_from_census'] = worksite_df.apply(lambda x: fips_choice('', x.fips_from_zip, x.fips_from_city), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining locations, find county using Google's Places API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by finding the Place ID for each location using Find Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the entries we need to use the API for\n",
    "whd_unmatched = worksite_df[(worksite_df['fips_from_census'] == '') & (worksite_df['city'] != '') & (worksite_df['state'] != '')][['city', 'state', 'zip']]\n",
    "whd_unmatched['state_name'] = whd_unmatched['state'].apply(lambda x: us.states.lookup(x).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ID for each row to link with API request responses\n",
    "whd_unmatched['id'] = whd_unmatched.reset_index().index.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split API calls into chunks of 100\n",
    "whd_unmatched['chunk'] = whd_unmatched['id'].astype(int)//100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google maps API key from my account\n",
    "# Import API key stored in text file\n",
    "with open(\"../tools/google_places_api_key.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "api_key = lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Base url to call Find Place API\n",
    "# base_url = \"https://maps.googleapis.com/maps/api/place/findplacefromtext/json?\"\n",
    "\n",
    "# for c in range(0, 1):\n",
    "#     whd_chunk = whd_unmatched[whd_unmatched['chunk'] == c]\n",
    "\n",
    "#     # Dict to store API responses\n",
    "#     api_placeid_dict = {}\n",
    "\n",
    "#     for ind in range(0, len(whd_chunk)):\n",
    "#         row = whd_chunk.iloc[ind]\n",
    "#         id = row['id']\n",
    "#         state_name = row['state_name']\n",
    "#         place_name = row['city']\n",
    "#         name_to_search = place_name + ', ' + state_name\n",
    "\n",
    "#         print(id, name_to_search)\n",
    "\n",
    "#         # Create API request\n",
    "#         # URL'ed location name we want to search\n",
    "#         input = urllib.parse.quote(name_to_search) # Encode place name as URL string\n",
    "#         request_url = base_url + \"input=\" + input + \"&inputtype=textquery\" + \"&fields=place_id\" + \"&key=\" + api_key\n",
    "\n",
    "#         payload = {}\n",
    "#         headers = {}\n",
    "\n",
    "#         # Sleep one second between each API call\n",
    "#         time.sleep(1)\n",
    "\n",
    "#         # Make API call\n",
    "#         response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#         response_json = response.json()\n",
    "        \n",
    "#         # If API call is successful, then place response result into dict\n",
    "#         if response_json['status']=='OK':\n",
    "#             print('Successful')\n",
    "#             api_placeid_dict[id] = response_json\n",
    "#         else:\n",
    "#             # If API call is unsuccessful, then wait 5 seconds and retry\n",
    "#             print('NOT successful, retrying')\n",
    "#             time.sleep(5)\n",
    "#             response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#             response_json = response.json()\n",
    "\n",
    "#             if response_json['status']=='OK':\n",
    "#                 print('Retry successful')\n",
    "#                 api_placeid_dict[id] = response_json\n",
    "#             else:\n",
    "#                 error_type = response_json['status']\n",
    "#                 print('Retry unsuccessful, error: ' + error_type)\n",
    "\n",
    "#     # Save API request results as JSON\n",
    "#     with open(f'json/whd_placeid_api_request_result_chunk_{c}.json', 'w') as f:\n",
    "#         json.dump(api_placeid_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON of API responses and put into DataFrame\n",
    "api_placeid_dict = {}\n",
    "for c in range(0, 1):\n",
    "    with open(f'json/whd_placeid_api_request_result_chunk_{c}.json', 'r') as infile:\n",
    "        api_dict = json.load(infile)\n",
    "\n",
    "    api_placeid_dict = api_placeid_dict | api_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put place IDs into DataFrame\n",
    "api_placeid_df = pd.DataFrame(columns=['id', 'placeid'])\n",
    "\n",
    "for id, response in api_placeid_dict.items():\n",
    "    number_of_candidates = len(response['candidates'])\n",
    "    for response_ind in range(0, number_of_candidates):\n",
    "        placeid = response['candidates'][response_ind]['place_id']\n",
    "        api_placeid_df.loc[len(api_placeid_df)] = [id, placeid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split API calls into chunks of 100\n",
    "api_placeid_df['chunk'] = api_placeid_df['id'].astype(int)//100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use Place details API to get county names\n",
    "# base_url = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "# for c in range(0, 1):\n",
    "#     api_placeid_chunk = api_placeid_df[api_placeid_df['chunk'] == c]\n",
    "#     api_place_details_dict = {}\n",
    "\n",
    "#     # Iterate over each place ID\n",
    "#     for index, row in api_placeid_chunk.iterrows():\n",
    "#         print(row['id'], row['placeid'])\n",
    "\n",
    "#         # Create API request\n",
    "#         input = row['placeid']\n",
    "#         request_url = base_url + \"place_id=\" + input + \"&key=\" + api_key\n",
    "\n",
    "#         payload = {}\n",
    "#         headers = {}\n",
    "\n",
    "#         response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#         response_json = response.json()\n",
    "\n",
    "#         # If API call is successful, then place response result into dict\n",
    "#         if response_json['status']=='OK':\n",
    "#             print('Successful')\n",
    "#             api_place_details_dict[input] = response_json\n",
    "#         else:\n",
    "#             # If API call is unsuccessful, then wait 5 seconds and retry\n",
    "#             print('NOT successful, retrying')\n",
    "#             time.sleep(5)\n",
    "#             response = requests.request(\"GET\", request_url, headers=headers, data=payload)\n",
    "#             response_json = response.json()\n",
    "\n",
    "#             if response_json['status']=='OK':\n",
    "#                 print('Retry successful')\n",
    "#                 api_place_details_dict[input] = response_json\n",
    "#             else:\n",
    "#                 error_type = response_json['status']\n",
    "#                 print('Retry unsuccessful, error: ' + error_type)\n",
    "\n",
    "#     # Save API request results as JSON\n",
    "#     with open(f'json/whd_place_details_api_request_result_chunk_{c}.json', 'w') as f:\n",
    "#         json.dump(api_place_details_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON of API responses and put into DataFrame\n",
    "api_place_details_dict = {}\n",
    "for c in range(0, 1):\n",
    "    with open(f'json/whd_place_details_api_request_result_chunk_{c}.json', 'r') as infile:\n",
    "        api_dict = json.load(infile)\n",
    "\n",
    "    api_place_details_dict = api_place_details_dict | api_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store county name from place details into dictionary (store state names too as there may be incorrect states)\n",
    "county_name_dict = {}\n",
    "state_name_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information we want from API response\n",
    "for placeid, response in api_place_details_dict.items():\n",
    "    n_responses = len(response['results'])\n",
    "\n",
    "    for response_ind in range(0, n_responses):\n",
    "        individual_response = response['results'][response_ind]\n",
    "        response_address_components_list = individual_response['address_components']\n",
    "        n_components = len(response_address_components_list)\n",
    "\n",
    "        for component_ind in range(0, n_components):\n",
    "            component_dict = response_address_components_list[component_ind]\n",
    "            component_type =  component_dict['types'][0]\n",
    "\n",
    "            if component_type == 'administrative_area_level_2':\n",
    "                county_name = component_dict['long_name']\n",
    "                county_name_dict[placeid] = county_name\n",
    "            \n",
    "            if component_type == 'administrative_area_level_1':\n",
    "                state_name = component_dict['long_name']\n",
    "                state_name_dict[placeid] = state_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add county and state name columns to Place ID\n",
    "api_placeid_df['county_name_api'] = api_placeid_df['placeid'].map(county_name_dict)\n",
    "api_placeid_df['state_name_api'] = api_placeid_df['placeid'].map(state_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of these multiple responses per place name are in the same county, so we can collapse those\n",
    "api_placeid_df = api_placeid_df.drop_duplicates(subset = ['id', 'county_name_api', 'state_name_api'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the remainder, manually resolve\n",
    "api_placeid_df = api_placeid_df.merge(whd_unmatched[['city', 'state_name', 'id']], how = 'left', on = ['id'])\n",
    "multiple_response = api_placeid_df[api_placeid_df.duplicated(subset=['id'], keep=False)]\n",
    "multiple_response.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recollapse after fixing\n",
    "api_placeid_df = api_placeid_df.drop_duplicates(subset = ['id', 'county_name_api', 'state_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get FIPS codes using addFIPS\n",
    "af = addfips.AddFIPS()\n",
    "api_placeid_df = api_placeid_df[~api_placeid_df['county_name_api'].isna()].copy()\n",
    "api_placeid_df['fips_api'] = api_placeid_df.apply(lambda x: af.get_county_fips(x['county_name_api'], state=x['state_name']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop API results that don't match states\n",
    "api_placeid_df = api_placeid_df[api_placeid_df['state_name'] == api_placeid_df['state_name_api']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recollapse back into individual entries (some entries had multiple places per entry)\n",
    "whd_api_df = whd_unmatched.merge(api_placeid_df[['id', 'county_name_api', 'fips_api']], how = 'left', on = ['id'])\n",
    "whd_api_df = whd_api_df[~whd_api_df['fips_api'].isna()].copy()\n",
    "whd_api_df = whd_api_df.groupby(['city', 'state', 'zip']).agg({'fips_api': lambda x: \",\".join(x)}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FIPS from API back to original list of worksites\n",
    "worksite_df = worksite_df.merge(whd_api_df, how = 'left', on = ['city', 'state', 'zip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "worksite_df = worksite_df.fillna(value='')\n",
    "worksite_df['fips'] = worksite_df['fips_from_census']\n",
    "worksite_df.loc[worksite_df['fips'] == '', 'fips'] = worksite_df.loc[worksite_df['fips'] == '', 'fips_api']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FIPS to H-2A entries based on worksites\n",
    "whd_final = whd_df.merge(worksite_df[['city', 'state', 'zip', 'fips']], how = 'left', on = ['city', 'state', 'zip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export binary\n",
    "whd_final.to_parquet(\"../binaries/whd_with_fips.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h-2a-paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
